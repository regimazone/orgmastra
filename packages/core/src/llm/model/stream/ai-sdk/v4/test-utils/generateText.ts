import assert from 'node:assert';
import type { LanguageModelV1CallOptions, LanguageModelV1CallWarning } from '@ai-sdk/provider';
import { jsonSchema } from '@ai-sdk/ui-utils';
import { Output } from 'ai';
import type { LanguageModelV1, LanguageModelV1StreamPart } from 'ai';
import { convertArrayToReadableStream, MockLanguageModelV1, mockId } from 'ai/test';
import { beforeEach, describe, expect, it, vi } from 'vitest';
import { z } from 'zod';
import type { execute, ExecuteParams } from '../../../execute';

function createTestModel({
  stream = convertArrayToReadableStream([
    {
      type: 'response-metadata',
      id: 'id-0',
      modelId: 'mock-model-id',
      timestamp: new Date(0),
    },
    { type: 'text-delta', textDelta: 'Hello' },
    { type: 'text-delta', textDelta: ', ' },
    { type: 'text-delta', textDelta: `world!` },
    {
      type: 'finish',
      finishReason: 'stop',
      logprobs: undefined,
      usage: { completionTokens: 10, promptTokens: 3 },
    },
  ]),
  rawCall = { rawPrompt: 'prompt', rawSettings: {} },
  rawResponse = undefined,
  request = undefined,
  warnings,
}: {
  stream?: ReadableStream<LanguageModelV1StreamPart>;
  rawResponse?: { headers: Record<string, string> };
  rawCall?: { rawPrompt: string; rawSettings: Record<string, unknown> };
  request?: { body: string };
  warnings?: LanguageModelV1CallWarning[];
} = {}): LanguageModelV1 {
  return new MockLanguageModelV1({
    doStream: async () => ({ stream, rawCall, rawResponse, request, warnings }),
  });
}

const createMockModelWithReasoning = () =>
  createTestModel({
    stream: convertArrayToReadableStream([
      {
        type: 'response-metadata',
        id: 'id-0',
        modelId: 'mock-model-id',
        timestamp: new Date(0),
      },
      { type: 'reasoning', textDelta: 'I will open the conversation' },
      { type: 'reasoning', textDelta: ' with witty banter.' },
      { type: 'reasoning-signature', signature: 'signature' },
      { type: 'redacted-reasoning', data: 'redacted-reasoning-data' },
      { type: 'reasoning-signature', signature: '1234567890' },
      { type: 'text-delta', textDelta: 'Hello, ' },
      { type: 'text-delta', textDelta: 'world!' },
      {
        type: 'finish',
        finishReason: 'stop',
        logprobs: undefined,
        usage: { completionTokens: 20, promptTokens: 10 },
      },
    ]),
  });

const createMockModelWithSources = () =>
  createTestModel({
    stream: convertArrayToReadableStream([
      {
        type: 'source',
        source: {
          sourceType: 'url' as const,
          id: '123',
          url: 'https://example.com',
          title: 'Example',
          providerMetadata: { provider: { custom: 'value' } },
        },
      },
      { type: 'text-delta', textDelta: 'Hello, world!' },
      {
        type: 'source',
        source: {
          sourceType: 'url' as const,
          id: '456',
          url: 'https://example.com/2',
          title: 'Example 2',
          providerMetadata: { provider: { custom: 'value2' } },
        },
      },
      {
        type: 'finish',
        finishReason: 'stop',
        logprobs: undefined,
        usage: { completionTokens: 20, promptTokens: 10 },
        // providerMetadata: { testprovider: { testkey: 'testvalue' } },
      },
    ]),
  });

const createMockModelWithFiles = () =>
  createTestModel({
    stream: convertArrayToReadableStream([
      {
        type: 'file',
        data: new Uint8Array([1, 2, 3]),
        mimeType: 'image/png',
      },
      {
        type: 'file',
        data: 'QkFVRw==',
        mimeType: 'image/jpeg',
      },
      { type: 'text-delta', textDelta: 'Hello, world!' },
      {
        type: 'finish',
        finishReason: 'stop',
        logprobs: undefined,
        usage: { completionTokens: 20, promptTokens: 10 },
      },
    ]),
  });

// Simple type assertion utility for testing type inference
function assertType<_T>(_value: _T): void {}

// Mock error types
class ToolExecutionError extends Error {
  constructor(options: { toolName: string; toolCallId: string; toolArgs: any; cause: Error }) {
    super(`Error executing tool ${options.toolName}: ${options.cause.message}`);
  }
}

// Mock tracer for telemetry tests
class MockTracer {
  jsonSpans: any[] = [];
}

const dummyResponseValues = {
  rawCall: { rawPrompt: 'prompt', rawSettings: {} },
  finishReason: 'stop' as const,
  usage: { promptTokens: 10, completionTokens: 20 },
};

export function generateTextTestsV4({ executeFn, runId }: { executeFn: typeof execute; runId: string }) {
  const generateText = async (args: Omit<ExecuteParams, 'runId'>) => {
    const output = await executeFn({
      runId,
      ...args,
    });
    return output.aisdk.v4.getFullOutput();
  };

  describe('generateText', () => {
    describe('result.text', () => {
      it('should generate text', async () => {
        const result = await generateText({
          model: new MockLanguageModelV1({
            /**
             * (calebbarnes) I added doStream to the mock here, the original test only mocked doGenerate
             * I'm assuming all v1 models support both doGenerate and doStream
             * The execute function uses doStream and then consumes the stream to return the text for generateText
             */
            doStream: async ({ prompt, mode }) => {
              expect(mode).toStrictEqual({
                type: 'regular',
                tools: undefined,
                toolChoice: undefined,
              });

              expect(prompt).toStrictEqual([
                {
                  role: 'user',
                  content: [{ type: 'text', text: 'prompt' }],
                  // providerMetadata: undefined, // TODO: the original expect included undefined keys
                },
              ]);

              return {
                stream: convertArrayToReadableStream([
                  { type: 'text-delta', textDelta: 'Hello, world!' },
                  {
                    type: 'finish',
                    finishReason: 'stop',
                    logprobs: undefined,
                    usage: { completionTokens: 10, promptTokens: 3 },
                  },
                ]),
                rawCall: { rawPrompt: 'prompt', rawSettings: {} },
              };
            },
          }),
          prompt: 'prompt',
        });

        expect(result.text).toStrictEqual('Hello, world!');
      });
    });
    describe('result.reasoning', () => {
      it('should contain reasoning string from model response', async () => {
        const result = await generateText({
          model: createMockModelWithReasoning(),
          prompt: 'prompt',
        });

        expect(result.reasoning).toStrictEqual('I will open the conversation with witty banter.');
      });
    });
    describe('result.sources', () => {
      it('should contain sources', async () => {
        const result = await generateText({
          model: createMockModelWithSources(),
          prompt: 'prompt',
        });

        expect(result.sources).toMatchSnapshot();
      });
    });
    describe('result.files', () => {
      it('should contain files', async () => {
        const result = await generateText({
          model: createMockModelWithFiles(),
          prompt: 'prompt',
        });

        expect(result.files).toMatchSnapshot();
      });
    });

    describe('result.steps', () => {
      it('should add the reasoning from the model response to the step result', async () => {
        const result = await generateText({
          model: createMockModelWithReasoning(),
          prompt: 'prompt',
          experimental_generateMessageId: mockId({
            prefix: 'msg',
          }),
          _internal: {
            generateId: mockId({ prefix: 'id' }),
            currentDate: () => new Date(0),
          },
        });

        expect(result.steps).toMatchSnapshot();
      });

      it('should contain sources', async () => {
        const result = await generateText({
          model: createMockModelWithSources(),
          prompt: 'prompt',
          experimental_generateMessageId: mockId({ prefix: 'msg' }),
          _internal: {
            generateId: mockId({ prefix: 'id' }),
            currentDate: () => new Date(0),
          },
        });

        expect(result.steps).toMatchSnapshot();
      });

      it('should contain files', async () => {
        const result = await generateText({
          model: createMockModelWithFiles(),
          prompt: 'prompt',
          experimental_generateMessageId: mockId({ prefix: 'msg' }),
          _internal: {
            generateId: mockId({ prefix: 'id' }),
            currentDate: () => new Date(0),
          },
        });

        expect(result.steps).toMatchSnapshot();
      });
    });

    describe('result.toolCalls', () => {
      it('should contain tool calls', async () => {
        const result = await generateText({
          model: new MockLanguageModelV1({
            doStream: async ({ prompt, mode }) => {
              // this is annoying it never shows the error in the test?
              assert.deepStrictEqual(mode, {
                type: 'regular',
                toolChoice: { type: 'required' },
                tools: [
                  {
                    type: 'function',
                    name: 'tool1',
                    description: undefined,
                    parameters: {
                      additionalProperties: false,
                      properties: { value: { type: 'string' } },
                      required: ['value'],
                      type: 'object',
                      $schema: 'http://json-schema.org/draft-07/schema#', // TODO: <-- this was not here before
                    },
                  },
                  {
                    type: 'function',
                    name: 'tool2',
                    description: undefined,
                    parameters: {
                      additionalProperties: false,
                      properties: { somethingElse: { type: 'string' } },
                      required: ['somethingElse'],
                      type: 'object',
                      $schema: 'http://json-schema.org/draft-07/schema#',
                    },
                  },
                ],
              });

              expect(prompt).toStrictEqual([
                {
                  role: 'user',
                  content: [{ type: 'text', text: 'test-input' }],
                  //   providerMetadata: undefined, // TODO: the original expect included undefined keys
                },
              ]);

              return {
                stream: convertArrayToReadableStream([
                  {
                    type: 'response-metadata',
                    id: 'id-0',
                    modelId: 'mock-model-id',
                    timestamp: new Date(0),
                  },
                  {
                    type: 'tool-call',
                    toolCallType: 'function',
                    toolCallId: 'call-1',
                    toolName: 'tool1',
                    args: `{ "value": "value" }`,
                  },
                  {
                    type: 'finish',
                    finishReason: 'stop',
                    logprobs: undefined,
                    usage: { completionTokens: 10, promptTokens: 3 },
                  },
                ]),
                rawCall: { rawPrompt: 'prompt', rawSettings: {} },
              };
            },
          }),
          tools: {
            tool1: {
              parameters: z.object({ value: z.string() }),
            },
            // 2nd tool to show typing:
            tool2: {
              parameters: z.object({ somethingElse: z.string() }),
            },
          },
          toolChoice: 'required',
          prompt: 'test-input',
        });

        // test type inference
        if (result.toolCalls[0].toolName === 'tool1') {
          assertType<string>(result.toolCalls[0].args.value);
        }

        expect(result.toolCalls).toStrictEqual([
          {
            type: 'tool-call',
            toolCallId: 'call-1',
            toolName: 'tool1',
            args: { value: 'value' },
          },
        ]);
      });
    });

    describe('result.toolResults', () => {
      it('should contain tool results', async () => {
        const result = await generateText({
          model: new MockLanguageModelV1({
            doStream: async ({ prompt, mode }) => {
              expect(mode).toStrictEqual({
                type: 'regular',
                toolChoice: { type: 'auto' },
                tools: [
                  {
                    type: 'function',
                    name: 'tool1',
                    description: undefined,
                    parameters: {
                      $schema: 'http://json-schema.org/draft-07/schema#',
                      additionalProperties: false,
                      properties: { value: { type: 'string' } },
                      required: ['value'],
                      type: 'object',
                    },
                  },
                ],
              });

              expect(prompt).toStrictEqual([
                {
                  role: 'user',
                  content: [{ type: 'text', text: 'test-input' }],
                  //   providerMetadata: undefined,
                },
              ]);

              return {
                stream: convertArrayToReadableStream([
                  {
                    type: 'response-metadata',
                    id: 'id-0',
                    modelId: 'mock-model-id',
                    timestamp: new Date(0),
                  },
                  {
                    type: 'tool-call',
                    toolCallType: 'function',
                    toolCallId: 'call-1',
                    toolName: 'tool1',
                    args: `{ "value": "value" }`,
                  },
                  {
                    type: 'finish',
                    finishReason: 'stop',
                    logprobs: undefined,
                    usage: { completionTokens: 10, promptTokens: 3 },
                  },
                ]),
                rawCall: { rawPrompt: 'prompt', rawSettings: {} },
              };
            },
          }),
          tools: {
            tool1: {
              parameters: z.object({ value: z.string() }),
              execute: async (args: any) => {
                expect(args).toStrictEqual({ value: 'value' });
                return 'result1';
              },
            },
          },
          prompt: 'test-input',
        });

        // test type inference
        if (result.toolResults[0].toolName === 'tool1') {
          assertType<string>(result.toolResults[0].result);
        }

        expect(result.toolResults).toStrictEqual([
          {
            type: 'tool-result',
            toolCallId: 'call-1',
            toolName: 'tool1',
            args: { value: 'value' },
            result: 'result1',
          },
        ]);
      });
    });

    describe('result.providerMetadata', () => {
      it('should contain provider metadata', async () => {
        const result = await generateText({
          model: new MockLanguageModelV1({
            doStream: async () => ({
              stream: convertArrayToReadableStream([
                {
                  type: 'response-metadata',
                  id: 'id-0',
                  modelId: 'mock-model-id',
                  timestamp: new Date(0),
                },
                {
                  type: 'finish',
                  finishReason: 'stop',
                  logprobs: undefined,
                  usage: { completionTokens: 10, promptTokens: 3 },
                  providerMetadata: {
                    anthropic: {
                      cacheCreationInputTokens: 10,
                      cacheReadInputTokens: 20,
                    },
                  },
                },
              ]),
              rawCall: { rawPrompt: 'prompt', rawSettings: {} },
            }),
          }),
          prompt: 'test-input',
        });

        expect(result.providerMetadata).toStrictEqual({
          anthropic: {
            cacheCreationInputTokens: 10,
            cacheReadInputTokens: 20,
          },
        });
      });
    });

    describe('result.response.messages', () => {
      it('should contain assistant response message when there are no tool calls', async () => {
        const result = await generateText({
          model: createTestModel({
            stream: convertArrayToReadableStream([
              {
                type: 'response-metadata',
                id: 'id-0',
                modelId: 'mock-model-id',
                timestamp: new Date(0),
              },
              { type: 'text-delta', textDelta: 'Hello, ' },
              { type: 'text-delta', textDelta: 'world!' },
              {
                type: 'finish',
                finishReason: 'stop',
                logprobs: undefined,
                usage: { completionTokens: 10, promptTokens: 3 },
              },
            ]),
          }),

          prompt: 'test-input',
          experimental_generateMessageId: mockId({ prefix: 'msg' }),
        });

        expect(result.response.messages).toMatchSnapshot();
      });

      // TODO: ID not incrementing for result.response.messages when there are test messages and tool calls, both have id msg-0 instead of 0 and 1
      it.todo(
        'should contain assistant response message and tool message when there are tool calls with results',
        async () => {
          const result = await generateText({
            model: createTestModel({
              stream: convertArrayToReadableStream([
                {
                  type: 'response-metadata',
                  id: 'id-0',
                  modelId: 'mock-model-id',
                  timestamp: new Date(0),
                },
                { type: 'text-delta', textDelta: 'Hello, ' },
                { type: 'text-delta', textDelta: 'world!' },
                {
                  type: 'tool-call',
                  toolCallType: 'function',
                  toolCallId: 'call-1',
                  toolName: 'tool1',
                  args: `{ "value": "value" }`,
                },
                {
                  type: 'finish',
                  finishReason: 'stop',
                  logprobs: undefined,
                  usage: { completionTokens: 10, promptTokens: 3 },
                },
              ]),
            }),
            tools: {
              tool1: {
                parameters: z.object({ value: z.string() }),
                execute: async (args: any, options: any) => {
                  expect(args).toStrictEqual({ value: 'value' });
                  expect(options.messages).toStrictEqual([{ role: 'user', content: 'test-input' }]);
                  return 'result1';
                },
              },
            },
            prompt: 'test-input',
            experimental_generateMessageId: mockId({ prefix: 'msg' }),
          });

          // expect(result.response.messages).toEqual('asd');
          expect(result.response.messages).toMatchSnapshot();
        },
      );

      it('should contain reasoning', async () => {
        const result = await generateText({
          model: createMockModelWithReasoning(),
          prompt: 'test-input',
          experimental_generateMessageId: mockId({ prefix: 'msg' }),
        });

        expect(result.response.messages).toMatchSnapshot();
      });
    });

    describe('result.request', () => {
      it('should contain request body', async () => {
        const result = await generateText({
          model: createTestModel({
            request: { body: 'test body' },
          }),
          prompt: 'prompt',
        });

        expect(result.request).toStrictEqual({
          body: 'test body',
        });
      });
    });

    // TODO: does not populate body on a streaming response -- we can track body ourselves
    describe.todo('result.response', () => {
      it('should contain response body and headers', async () => {
        const result = await generateText({
          model: new MockLanguageModelV1({
            doStream: async () => ({
              stream: convertArrayToReadableStream([
                {
                  type: 'response-metadata',
                  id: 'test-id-from-model',
                  modelId: 'test-response-model-id',
                  timestamp: new Date(10000),
                },
                { type: 'text-delta', textDelta: 'Hello, ' },
                { type: 'text-delta', textDelta: 'world!' },
                {
                  type: 'finish',
                  finishReason: 'stop',
                  logprobs: undefined,
                  usage: { completionTokens: 10, promptTokens: 3 },
                },
              ]),
              rawCall: { rawPrompt: 'prompt', rawSettings: {} },

              rawResponse: {
                headers: {
                  'custom-response-header': 'response-header-value',
                },
                body: 'test body', // TODO: <-- does not populate body on a streaming response
              },
            }),
          }),
          prompt: 'prompt',
          experimental_generateMessageId: mockId({ prefix: 'msg' }),
        });

        expect(result.steps[0].response).toMatchSnapshot();
        expect(result.response).toMatchSnapshot();
      });
    });

    describe('options.headers', () => {
      it('should pass headers to model', async () => {
        const result = await generateText({
          model: new MockLanguageModelV1({
            doStream: async ({ headers }) => {
              assert.deepStrictEqual(headers, {
                'custom-request-header': 'request-header-value',
              });

              return {
                stream: convertArrayToReadableStream([
                  {
                    type: 'response-metadata',
                    id: 'test-id-from-model',
                    modelId: 'test-response-model-id',
                    timestamp: new Date(10000),
                  },
                  { type: 'text-delta', textDelta: 'Hello, ' },
                  { type: 'text-delta', textDelta: 'world!' },
                  {
                    type: 'finish',
                    finishReason: 'stop',
                    logprobs: undefined,
                    usage: { completionTokens: 10, promptTokens: 3 },
                  },
                ]),
                rawCall: { rawPrompt: 'prompt', rawSettings: {} },
              };
            },
          }),
          prompt: 'test-input',
          headers: {
            'custom-request-header': 'request-header-value',
          },
        });

        assert.deepStrictEqual(result.text, 'Hello, world!');
      });
    });

    describe('options.providerOptions', () => {
      it('should pass provider options to model', async () => {
        const result = await generateText({
          model: new MockLanguageModelV1({
            doStream: async ({ providerMetadata, ...rest }) => {
              console.log('providerMetadata222', providerMetadata); // undefined
              console.log('rest222', rest);
              expect(providerMetadata).toStrictEqual({
                aProvider: { someKey: 'someValue' },
              });
              return {
                stream: convertArrayToReadableStream([
                  {
                    type: 'response-metadata',
                    id: 'test-id-from-model',
                    modelId: 'test-response-model-id',
                    timestamp: new Date(10000),
                  },
                  { type: 'text-delta', textDelta: 'provider metadata test' },
                  {
                    type: 'finish',
                    finishReason: 'stop',
                    logprobs: undefined,
                    usage: { completionTokens: 10, promptTokens: 3 },
                  },
                ]),
                rawCall: { rawPrompt: 'prompt', rawSettings: {} },
              };
            },
          }),
          prompt: 'test-input',
          providerOptions: {
            aProvider: { someKey: 'someValue' },
          },
        });

        expect(result.text).toStrictEqual('provider metadata test');
      });
    });

    // todo abortSignal is not passed to execute
    describe.todo('options.abortSignal', () => {
      it('should forward abort signal to tool execution', async () => {
        const abortController = new AbortController();
        const toolExecuteMock = vi.fn().mockResolvedValue('tool result');

        const generateTextPromise = generateText({
          model: new MockLanguageModelV1({
            doStream: async () => ({
              stream: convertArrayToReadableStream([
                {
                  type: 'response-metadata',
                  id: 'id-0',
                  modelId: 'mock-model-id',
                  timestamp: new Date(0),
                },
                {
                  type: 'tool-call',
                  toolCallType: 'function',
                  toolCallId: 'call-1',
                  toolName: 'tool1',
                  args: `{ "value": "value" }`,
                },
                {
                  type: 'finish',
                  finishReason: 'stop',
                  logprobs: undefined,
                  usage: { completionTokens: 10, promptTokens: 3 },
                },
              ]),
              rawCall: { rawPrompt: 'prompt', rawSettings: {} },
            }),
          }),
          tools: {
            tool1: {
              parameters: z.object({ value: z.string() }),
              execute: toolExecuteMock,
            },
          },
          prompt: 'test-input',
          abortSignal: abortController.signal, // TODO <-- abortSignal is not passed to execute
        });

        // Abort the operation
        abortController.abort();

        await generateTextPromise;

        expect(toolExecuteMock).toHaveBeenCalledWith(
          { value: 'value' },
          {
            abortSignal: abortController.signal,
            toolCallId: 'call-1',
            messages: expect.any(Array),
          },
        );
      });
    });

    // todo -- telemetry options?
    describe.skip('telemetry', () => {
      let tracer: MockTracer;

      beforeEach(() => {
        tracer = new MockTracer();
      });

      it('should not record any telemetry data when not explicitly enabled', async () => {
        await generateText({
          model: new MockLanguageModelV1({
            doGenerate: async ({}) => ({
              ...dummyResponseValues,
              text: `Hello, world!`,
            }),
          }),
          prompt: 'prompt',
          experimental_telemetry: { tracer },
        });

        expect(tracer.jsonSpans).toMatchSnapshot();
      });

      it('should record telemetry data when enabled', async () => {
        await generateText({
          model: new MockLanguageModelV1({
            doGenerate: async ({}) => ({
              ...dummyResponseValues,
              text: `Hello, world!`,
              response: {
                id: 'test-id-from-model',
                timestamp: new Date(10000),
                modelId: 'test-response-model-id',
              },
              providerMetadata: {
                testprovider: {
                  testkey: 'testvalue',
                },
              },
            }),
          }),
          prompt: 'prompt',
          topK: 0.1,
          topP: 0.2,
          frequencyPenalty: 0.3,
          presencePenalty: 0.4,
          temperature: 0.5,
          stopSequences: ['stop'],
          headers: {
            header1: 'value1',
            header2: 'value2',
          },
          experimental_telemetry: {
            isEnabled: true,
            functionId: 'test-function-id',
            metadata: {
              test1: 'value1',
              test2: false,
            },
            tracer,
          },
        });

        expect(tracer.jsonSpans).toMatchSnapshot();
      });

      it('should record successful tool call', async () => {
        await generateText({
          model: new MockLanguageModelV1({
            doGenerate: async ({}) => ({
              ...dummyResponseValues,
              toolCalls: [
                {
                  toolCallType: 'function',
                  toolCallId: 'call-1',
                  toolName: 'tool1',
                  args: `{ "value": "value" }`,
                },
              ],
            }),
          }),
          tools: {
            tool1: {
              parameters: z.object({ value: z.string() }),
              execute: async () => 'result1',
            },
          },
          prompt: 'test-input',
          experimental_telemetry: {
            isEnabled: true,
            tracer,
          },
          _internal: {
            generateId: () => 'test-id',
            currentDate: () => new Date(0),
          },
        });

        expect(tracer.jsonSpans).toMatchSnapshot();
      });

      it('should record error on tool call', async () => {
        await expect(async () => {
          await generateText({
            model: new MockLanguageModelV1({
              doGenerate: async ({}) => ({
                ...dummyResponseValues,
                toolCalls: [
                  {
                    toolCallType: 'function',
                    toolCallId: 'call-1',
                    toolName: 'tool1',
                    args: `{ "value": "value" }`,
                  },
                ],
              }),
            }),
            tools: {
              tool1: {
                parameters: z.object({ value: z.string() }),
                execute: async () => {
                  throw new Error('test error');
                },
              },
            },
            prompt: 'test-input',
            experimental_telemetry: {
              isEnabled: true,
              tracer,
            },
            _internal: {
              generateId: () => 'test-id',
              currentDate: () => new Date(0),
            },
          });
        }).rejects.toThrow(ToolExecutionError);

        expect(tracer.jsonSpans).toHaveLength(3);

        const toolCallSpan = tracer.jsonSpans[2];
        expect(toolCallSpan.events).toHaveLength(2);
        expect(toolCallSpan.status).toEqual({
          code: 2,
          message: 'Error executing tool tool1: test error',
        });

        // Find the exception event
        const exceptionEvent = toolCallSpan.events.find(event => event.name === 'exception');
        expect(exceptionEvent).toBeDefined();
        expect(exceptionEvent!.attributes).toMatchObject({
          'exception.message': 'test error',
          'exception.name': 'Error',
          'exception.stack': expect.any(String),
        });
      });

      it('should not record telemetry inputs / outputs when disabled', async () => {
        await generateText({
          model: new MockLanguageModelV1({
            doGenerate: async ({}) => ({
              ...dummyResponseValues,
              toolCalls: [
                {
                  toolCallType: 'function',
                  toolCallId: 'call-1',
                  toolName: 'tool1',
                  args: `{ "value": "value" }`,
                },
              ],
            }),
          }),
          tools: {
            tool1: {
              parameters: z.object({ value: z.string() }),
              execute: async () => 'result1',
            },
          },
          prompt: 'test-input',
          experimental_telemetry: {
            isEnabled: true,
            recordInputs: false,
            recordOutputs: false,
            tracer,
          },
          _internal: {
            generateId: () => 'test-id',
            currentDate: () => new Date(0),
          },
        });

        expect(tracer.jsonSpans).toMatchSnapshot();
      });
    });

    describe('tools with custom schema', () => {
      it('should contain tool calls', async () => {
        const result = await generateText({
          model: new MockLanguageModelV1({
            doStream: async ({ prompt, mode }) => {
              assert.deepStrictEqual(mode, {
                type: 'regular',
                toolChoice: { type: 'required' },
                tools: [
                  {
                    type: 'function',
                    name: 'tool1',
                    description: undefined,
                    parameters: {
                      additionalProperties: false,
                      properties: { value: { type: 'string' } },
                      required: ['value'],
                      type: 'object',
                    },
                  },
                  {
                    type: 'function',
                    name: 'tool2',
                    description: undefined,
                    parameters: {
                      additionalProperties: false,
                      properties: { somethingElse: { type: 'string' } },
                      required: ['somethingElse'],
                      type: 'object',
                    },
                  },
                ],
              });

              expect(prompt).toStrictEqual([
                {
                  role: 'user',
                  content: [{ type: 'text', text: 'test-input' }],
                  //   providerMetadata: undefined,
                },
              ]);

              return {
                stream: convertArrayToReadableStream([
                  {
                    type: 'response-metadata',
                    id: 'id-0',
                    modelId: 'mock-model-id',
                    timestamp: new Date(0),
                  },
                  {
                    type: 'tool-call',
                    toolCallType: 'function',
                    toolCallId: 'call-1',
                    toolName: 'tool1',
                    args: `{ "value": "value" }`,
                  },
                  {
                    type: 'finish',
                    finishReason: 'stop',
                    logprobs: undefined,
                    usage: { completionTokens: 10, promptTokens: 3 },
                  },
                ]),
                rawCall: { rawPrompt: 'prompt', rawSettings: {} },
              };
            },
          }),
          tools: {
            tool1: {
              parameters: jsonSchema<{ value: string }>({
                type: 'object',
                properties: { value: { type: 'string' } },
                required: ['value'],
                additionalProperties: false,
              }),
            },
            // 2nd tool to show typing:
            tool2: {
              parameters: jsonSchema<{ somethingElse: string }>({
                type: 'object',
                properties: { somethingElse: { type: 'string' } },
                required: ['somethingElse'],
                additionalProperties: false,
              }),
            },
          },
          toolChoice: 'required',
          prompt: 'test-input',
          _internal: {
            generateId: () => 'test-id',
            currentDate: () => new Date(0),
          },
        });

        // test type inference
        if (result.toolCalls[0].toolName === 'tool1') {
          assertType<string>(result.toolCalls[0].args.value);
        }

        assert.deepStrictEqual(result.toolCalls, [
          {
            type: 'tool-call',
            toolCallId: 'call-1',
            toolName: 'tool1',
            args: { value: 'value' },
          },
        ]);
      });
    });

    // todo options
    describe.skip('options.messages', () => {
      it('should detect and convert ui messages', async () => {
        const result = await generateText({
          model: new MockLanguageModelV1({
            doStream: async ({ prompt }) => {
              expect(prompt).toStrictEqual([
                {
                  content: [
                    {
                      text: 'prompt',
                      type: 'text',
                    },
                  ],
                  //   providerMetadata: undefined,
                  role: 'user',
                },
                {
                  content: [
                    {
                      args: {
                        value: 'test-value',
                      },
                      //   providerMetadata: undefined,
                      toolCallId: 'call-1',
                      toolName: 'test-tool',
                      type: 'tool-call',
                    },
                  ],
                  //   providerMetadata: undefined,
                  role: 'assistant',
                },
                {
                  content: [
                    {
                      //   content: undefined,
                      //   isError: undefined,
                      //   providerMetadata: undefined,
                      result: 'test result',
                      toolCallId: 'call-1',
                      toolName: 'test-tool',
                      type: 'tool-result',
                    },
                  ],
                  //   providerMetadata: undefined,
                  role: 'tool',
                },
              ]);

              return {
                stream: convertArrayToReadableStream([
                  {
                    type: 'response-metadata',
                    id: 'id-0',
                    modelId: 'mock-model-id',
                    timestamp: new Date(0),
                  },
                  { type: 'text-delta', textDelta: 'Hello, ' },
                  { type: 'text-delta', textDelta: 'world!' },
                  {
                    type: 'finish',
                    finishReason: 'stop',
                    logprobs: undefined,
                    usage: { completionTokens: 10, promptTokens: 3 },
                  },
                ]),
                rawCall: { rawPrompt: 'prompt', rawSettings: {} },
              };
            },
          }),
          messages: [
            {
              role: 'user',
              content: 'prompt',
            },
            {
              role: 'assistant',
              content: '',
              toolInvocations: [
                {
                  state: 'result',
                  toolCallId: 'call-1',
                  toolName: 'test-tool',
                  args: { value: 'test-value' },
                  result: 'test result',
                },
              ],
            },
          ],
        });

        expect(result.text).toStrictEqual('Hello, world!');
      });

      it('should support models that use "this" context in supportsUrl', async () => {
        let supportsUrlCalled = false;
        class MockLanguageModelWithImageSupport extends MockLanguageModelV1 {
          readonly supportsImageUrls = false;

          constructor() {
            super({
              supportsUrl(_url: URL) {
                supportsUrlCalled = true;
                // Reference 'this' to verify context
                return this.modelId === 'mock-model-id';
              },
              doStream: async () => ({
                stream: convertArrayToReadableStream([
                  {
                    type: 'response-metadata',
                    id: 'id-0',
                    modelId: 'mock-model-id',
                    timestamp: new Date(0),
                  },
                  { type: 'text-delta', textDelta: 'Hello, ' },
                  { type: 'text-delta', textDelta: 'world!' },
                  {
                    type: 'finish',
                    finishReason: 'stop',
                    logprobs: undefined,
                    usage: { completionTokens: 10, promptTokens: 3 },
                  },
                ]),
                rawCall: { rawPrompt: 'prompt', rawSettings: {} },
              }),
            });
          }
        }

        const model = new MockLanguageModelWithImageSupport();

        const result = await generateText({
          model,
          messages: [
            {
              role: 'user',
              content: [{ type: 'image', image: 'https://example.com/test.jpg' }],
            },
          ],
        });

        expect(result.text).toStrictEqual('Hello, world!');
        expect(supportsUrlCalled).toBe(true);
      });
    });

    // todo options
    describe.skip('options.output', () => {
      describe('no output', () => {
        it.todo('should throw error when accessing output', async () => {
          const result = await generateText({
            model: createTestModel(),
            prompt: 'prompt',
          });

          expect(() => {
            result.experimental_output;
          }).toThrow('No output specified');
        });
      });

      describe('text output', () => {
        it.todo('should forward text as output', async () => {
          const result = await generateText({
            model: createTestModel(),
            prompt: 'prompt',
            experimental_output: Output.text(),
          });
          // console.log('result5552', JSON.stringify(result, null, 2));
          expect(result.experimental_output).toStrictEqual('Hello, world!');
        });

        it.todo('should set responseFormat to text and not change the prompt', async () => {
          let callOptions: LanguageModelV1CallOptions;

          await generateText({
            model: new MockLanguageModelV1({
              doStream: async args => {
                callOptions = args;
                return {
                  stream: convertArrayToReadableStream([
                    {
                      type: 'response-metadata',
                      id: 'id-0',
                      modelId: 'mock-model-id',
                      timestamp: new Date(0),
                    },
                    { type: 'text-delta', textDelta: 'Hello, ' },
                    { type: 'text-delta', textDelta: 'world!' },
                    {
                      type: 'finish',
                      finishReason: 'stop',
                      logprobs: undefined,
                      usage: { completionTokens: 10, promptTokens: 3 },
                    },
                  ]),
                  rawCall: { rawPrompt: 'prompt', rawSettings: {} },
                };
              },
            }),
            prompt: 'prompt',
            experimental_output: Output.text(),
          });

          expect(callOptions!).toEqual({
            temperature: 0, // TODO: <-- missing
            mode: {
              toolChoice: undefined, // ! i added this. <-- was not in this expect originally
              tools: undefined, // ! i added this. <-- was not in this expect originally
              type: 'regular',
            },
            responseFormat: { type: 'text' }, // TODO: <-- missing
            inputFormat: 'prompt',
            prompt: [
              {
                content: [{ text: 'prompt', type: 'text' }],
                // providerMetadata: undefined, // TODO: missing
                role: 'user',
              },
            ],
            providerMetadata: undefined, // ! i added this. <-- was not in this expect originally
          });
        });
      });

      describe('object output', () => {
        describe('without structured output model', () => {
          // TODO: support experimental_output
          it.todo('should parse the output', async () => {
            const result = await generateText({
              model: new MockLanguageModelV1({
                supportsStructuredOutputs: false,
                doStream: async () => ({
                  stream: convertArrayToReadableStream([
                    {
                      type: 'response-metadata',
                      id: 'id-0',
                      modelId: 'mock-model-id',
                      timestamp: new Date(0),
                    },
                    { type: 'text-delta', textDelta: '{' },
                    { type: 'text-delta', textDelta: ' ' },
                    { type: 'text-delta', textDelta: '"value": ' },
                    { type: 'text-delta', textDelta: '"' },
                    { type: 'text-delta', textDelta: 'test-value' },
                    { type: 'text-delta', textDelta: '"' },
                    { type: 'text-delta', textDelta: '}' },
                    {
                      type: 'finish',
                      finishReason: 'stop',
                      logprobs: undefined,
                      usage: { completionTokens: 10, promptTokens: 3 },
                    },
                  ]),
                  rawCall: { rawPrompt: 'prompt', rawSettings: {} },
                }),
              }),
              prompt: 'prompt',
              experimental_output: Output.object({
                schema: z.object({ value: z.string() }),
              }),
            });

            expect(result.experimental_output).toEqual({ value: 'test-value' });
          });

          it.todo(
            'should set responseFormat to json and inject schema and JSON instruction into the prompt',
            async () => {
              let callOptions: LanguageModelV1CallOptions;

              await generateText({
                model: new MockLanguageModelV1({
                  supportsStructuredOutputs: false,
                  doStream: async args => {
                    callOptions = args;
                    return {
                      stream: convertArrayToReadableStream([
                        {
                          type: 'response-metadata',
                          id: 'id-0',
                          modelId: 'mock-model-id',
                          timestamp: new Date(0),
                        },
                        { type: 'text-delta', textDelta: '{' },
                        { type: 'text-delta', textDelta: ' ' },
                        { type: 'text-delta', textDelta: '"value": ' },
                        { type: 'text-delta', textDelta: '"' },
                        { type: 'text-delta', textDelta: 'test-value' },
                        { type: 'text-delta', textDelta: '"' },
                        { type: 'text-delta', textDelta: '}' },
                        {
                          type: 'finish',
                          finishReason: 'stop',
                          logprobs: undefined,
                          usage: { completionTokens: 10, promptTokens: 3 },
                        },
                      ]),
                      rawCall: { rawPrompt: 'prompt', rawSettings: {} },
                    };
                  },
                }),
                prompt: 'prompt',
                experimental_output: Output.object({
                  schema: z.object({ value: z.string() }),
                }),
              });

              expect(callOptions!).toEqual({
                temperature: 0,
                mode: { type: 'regular' },
                inputFormat: 'prompt',
                responseFormat: { type: 'json', schema: undefined },
                prompt: [
                  {
                    content:
                      'JSON schema:\n' +
                      '{"type":"object","properties":{"value":{"type":"string"}},"required":["value"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}\n' +
                      'You MUST answer with a JSON object that matches the JSON schema above.',
                    role: 'system',
                  },
                  {
                    content: [{ text: 'prompt', type: 'text' }],
                    providerMetadata: undefined,
                    role: 'user',
                  },
                ],
              });
            },
          );
        });

        describe('with structured output model', () => {
          it.todo('should parse the output', async () => {
            const result = await generateText({
              model: new MockLanguageModelV1({
                supportsStructuredOutputs: true,
                doStream: async () => ({
                  stream: convertArrayToReadableStream([
                    {
                      type: 'response-metadata',
                      id: 'id-0',
                      modelId: 'mock-model-id',
                      timestamp: new Date(0),
                    },
                    { type: 'text-delta', textDelta: '{' },
                    { type: 'text-delta', textDelta: ' ' },
                    { type: 'text-delta', textDelta: '"value": ' },
                    { type: 'text-delta', textDelta: '"' },
                    { type: 'text-delta', textDelta: 'test-value' },
                    { type: 'text-delta', textDelta: '"' },
                    { type: 'text-delta', textDelta: '}' },
                    {
                      type: 'finish',
                      finishReason: 'stop',
                      logprobs: undefined,
                      usage: { completionTokens: 10, promptTokens: 3 },
                    },
                  ]),
                  rawCall: { rawPrompt: 'prompt', rawSettings: {} },
                }),
              }),
              prompt: 'prompt',
              experimental_output: Output.object({
                schema: z.object({ value: z.string() }),
              }),
            });

            expect(result.experimental_output).toEqual({ value: 'test-value' });
          });

          it.todo('should set responseFormat to json and send schema as part of the responseFormat', async () => {
            let callOptions: LanguageModelV1CallOptions;

            await generateText({
              model: new MockLanguageModelV1({
                supportsStructuredOutputs: true,
                doStream: async args => {
                  callOptions = args;
                  return {
                    stream: convertArrayToReadableStream([
                      {
                        type: 'response-metadata',
                        id: 'id-0',
                        modelId: 'mock-model-id',
                        timestamp: new Date(0),
                      },
                      { type: 'text-delta', textDelta: '{ "value": "test-value" }' },
                      {
                        type: 'finish',
                        finishReason: 'stop',
                        logprobs: undefined,
                        usage: { completionTokens: 10, promptTokens: 3 },
                      },
                    ]),
                    rawCall: { rawPrompt: 'prompt', rawSettings: {} },
                  };
                },
              }),
              prompt: 'prompt',
              experimental_output: Output.object({
                schema: z.object({ value: z.string() }),
              }),
            });

            expect(callOptions!).toEqual({
              temperature: 0,
              mode: { type: 'regular' },
              inputFormat: 'prompt',
              responseFormat: {
                type: 'json',
                schema: {
                  $schema: 'http://json-schema.org/draft-07/schema#',
                  additionalProperties: false,
                  properties: { value: { type: 'string' } },
                  required: ['value'],
                  type: 'object',
                },
              },
              prompt: [
                {
                  content: [{ text: 'prompt', type: 'text' }],
                  providerMetadata: undefined,
                  role: 'user',
                },
              ],
            });
          });
        });
      });
    });

    // todo
    describe.todo('tool execution errors', () => {
      it('should throw a ToolExecutionError when a tool execution throws an error', async () => {
        await expect(async () => {
          await generateText({
            model: new MockLanguageModelV1({
              doGenerate: async () => ({
                ...dummyResponseValues,
                toolCalls: [
                  {
                    toolCallType: 'function',
                    toolCallId: 'call-1',
                    toolName: 'tool1',
                    args: `{ "value": "value" }`,
                  },
                ],
              }),
            }),
            tools: {
              tool1: {
                parameters: z.object({ value: z.string() }),
                execute: async () => {
                  throw new Error('test error');
                },
              },
            },
            prompt: 'test-input',
          });
        }).rejects.toThrow(
          new ToolExecutionError({
            toolName: 'tool1',
            toolCallId: 'call-1',
            toolArgs: { value: 'value' },
            cause: new Error('test error'),
          }),
        );
      });
    });

    // TODO -- options.maxSteps
    // describe.skip('options.maxSteps', () => {
    //   describe('2 steps: initial, tool-result', () => {
    //     let result: any; // GenerateTextResult<any, any>;
    //     let onStepFinishResults: StepResult<any>[];

    //     beforeEach(async () => {
    //       onStepFinishResults = [];

    //       let responseCount = 0;
    //       result = await generateText({
    //         model: new MockLanguageModelV1({
    //           doStream: async ({ prompt, mode }) => {
    //             switch (responseCount++) {
    //               case 0: {
    //                 // expect(mode).toStrictEqual({
    //                 //   type: 'regular',
    //                 //   toolChoice: { type: 'auto' },
    //                 //   tools: [
    //                 //     {
    //                 //       type: 'function',
    //                 //       name: 'tool1',
    //                 //     //   description: undefined,
    //                 //       parameters: {
    //                 //         $schema: 'http://json-schema.org/draft-07/schema#',
    //                 //         additionalProperties: false,
    //                 //         properties: { value: { type: 'string' } },
    //                 //         required: ['value'],
    //                 //         type: 'object',
    //                 //       },
    //                 //     },
    //                 //   ],
    //                 // });

    //                 expect(prompt).toStrictEqual([
    //                   {
    //                     role: 'user',
    //                     content: [{ type: 'text', text: 'test-input' }],
    //                     // providerMetadata: undefined,
    //                   },
    //                 ]);
    //                 return {
    //                   stream: convertArrayToReadableStream([
    //                     {
    //                       type: 'response-metadata',
    //                       id: 'id-0',
    //                       modelId: 'mock-model-id',
    //                       timestamp: new Date(0),
    //                     },
    //                     { type: 'text-delta', textDelta: 'Hello, ' },
    //                     { type: 'text-delta', textDelta: 'world!' },
    //                     {
    //                       type: 'finish',
    //                       finishReason: 'stop',
    //                       logprobs: undefined,
    //                       usage: { completionTokens: 10, promptTokens: 3 },
    //                     },
    //                   ]),
    //                   rawCall: { rawPrompt: 'prompt', rawSettings: {} },
    //                 };
    //               }
    //               case 1: {
    //                 return {
    //                   stream: convertArrayToReadableStream([
    //                     {
    //                       type: 'response-metadata',
    //                       id: 'id-1',
    //                       modelId: 'mock-model-id',
    //                       timestamp: new Date(1000),
    //                     },
    //                     { type: 'text-delta', textDelta: 'Hello, ' },
    //                     { type: 'text-delta', textDelta: `world!` },
    //                     {
    //                       type: 'finish',
    //                       finishReason: 'stop',
    //                       logprobs: undefined,
    //                       usage: { completionTokens: 5, promptTokens: 1 },
    //                     },
    //                   ]),
    //                   rawCall: { rawPrompt: 'prompt', rawSettings: {} },
    //                 };
    //               }
    //             }
    //           },
    //           //   doGenerate: async ({ prompt, mode }) => {
    //           //     switch (responseCount++) {
    //           //       case 0:
    //           //         expect(mode).toStrictEqual({
    //           //           type: 'regular',
    //           //           toolChoice: { type: 'auto' },
    //           //           tools: [
    //           //             {
    //           //               type: 'function',
    //           //               name: 'tool1',
    //           //               description: undefined,
    //           //               parameters: {
    //           //                 $schema: 'http://json-schema.org/draft-07/schema#',
    //           //                 additionalProperties: false,
    //           //                 properties: { value: { type: 'string' } },
    //           //                 required: ['value'],
    //           //                 type: 'object',
    //           //               },
    //           //             },
    //           //           ],
    //           //         });

    //           //         expect(prompt).toStrictEqual([
    //           //           {
    //           //             role: 'user',
    //           //             content: [{ type: 'text', text: 'test-input' }],
    //           //             providerMetadata: undefined,
    //           //           },
    //           //         ]);

    //           //         return {
    //           //           ...dummyResponseValues,
    //           //           toolCalls: [
    //           //             {
    //           //               toolCallType: 'function',
    //           //               toolCallId: 'call-1',
    //           //               toolName: 'tool1',
    //           //               args: `{ "value": "value" }`,
    //           //             },
    //           //           ],
    //           //           toolResults: [
    //           //             {
    //           //               toolCallId: 'call-1',
    //           //               toolName: 'tool1',
    //           //               args: { value: 'value' },
    //           //               result: 'result1',
    //           //             },
    //           //           ],
    //           //           finishReason: 'tool-calls',
    //           //           usage: { completionTokens: 5, promptTokens: 10 },
    //           //           response: {
    //           //             id: 'test-id-1-from-model',
    //           //             timestamp: new Date(0),
    //           //             modelId: 'test-response-model-id',
    //           //           },
    //           //         };
    //           //       case 1:
    //           //         expect(mode).toStrictEqual({
    //           //           type: 'regular',
    //           //           toolChoice: { type: 'auto' },
    //           //           tools: [
    //           //             {
    //           //               type: 'function',
    //           //               name: 'tool1',
    //           //               description: undefined,
    //           //               parameters: {
    //           //                 $schema: 'http://json-schema.org/draft-07/schema#',
    //           //                 additionalProperties: false,
    //           //                 properties: { value: { type: 'string' } },
    //           //                 required: ['value'],
    //           //                 type: 'object',
    //           //               },
    //           //             },
    //           //           ],
    //           //         });

    //           //         expect(prompt).toStrictEqual([
    //           //           {
    //           //             role: 'user',
    //           //             content: [{ type: 'text', text: 'test-input' }],
    //           //             providerMetadata: undefined,
    //           //           },
    //           //           {
    //           //             role: 'assistant',
    //           //             content: [
    //           //               {
    //           //                 type: 'tool-call',
    //           //                 toolCallId: 'call-1',
    //           //                 toolName: 'tool1',
    //           //                 args: { value: 'value' },
    //           //                 providerMetadata: undefined,
    //           //               },
    //           //             ],
    //           //             providerMetadata: undefined,
    //           //           },
    //           //           {
    //           //             role: 'tool',
    //           //             content: [
    //           //               {
    //           //                 type: 'tool-result',
    //           //                 toolCallId: 'call-1',
    //           //                 toolName: 'tool1',
    //           //                 result: 'result1',
    //           //                 content: undefined,
    //           //                 isError: undefined,
    //           //                 providerMetadata: undefined,
    //           //               },
    //           //             ],
    //           //             providerMetadata: undefined,
    //           //           },
    //           //         ]);
    //           //         return {
    //           //           ...dummyResponseValues,
    //           //           text: 'Hello, world!',
    //           //           response: {
    //           //             id: 'test-id-2-from-model',
    //           //             timestamp: new Date(10000),
    //           //             modelId: 'test-response-model-id',
    //           //           },
    //           //           rawResponse: {
    //           //             headers: {
    //           //               'custom-response-header': 'response-header-value',
    //           //             },
    //           //           },
    //           //         };
    //           //       default:
    //           //         throw new Error(`Unexpected response count: ${responseCount}`);
    //           //     }
    //           //   },
    //         }),
    //         tools: {
    //           tool1: tool({
    //             parameters: z.object({ value: z.string() }),
    //             execute: async (args, options) => {
    //               expect(args).toStrictEqual({ value: 'value' });
    //               expect(options.messages).toStrictEqual([{ role: 'user', content: 'test-input' }]);
    //               return 'result1';
    //             },
    //           }),
    //         },
    //         prompt: 'test-input',
    //         maxSteps: 3,
    //         // TODO: ? onStepFinish does not exist
    //         // onStepFinish: async event => {
    //         //   onStepFinishResults.push(event);
    //         // },
    //         experimental_generateMessageId: mockId({ prefix: 'msg' }),
    //       });
    //     });

    //     it('result.text should return text from last step', async () => {
    //       assert.deepStrictEqual(result.text, 'Hello, world!');
    //     });

    //     it('result.toolCalls should return empty tool calls from last step', async () => {
    //       assert.deepStrictEqual(result.toolCalls, []);
    //     });

    //     it('result.toolResults should return empty tool results from last step', async () => {
    //       assert.deepStrictEqual(result.toolResults, []);
    //     });

    //     it('result.response.messages should contain response messages from all steps', () => {
    //       expect(result.response.messages).toMatchSnapshot();
    //     });

    //     it('result.usage should sum token usage', () => {
    //       assert.deepStrictEqual(result.usage, {
    //         completionTokens: 25,
    //         promptTokens: 20,
    //         totalTokens: 45,
    //       });
    //     });

    //     it('result.steps should contain all steps', () => {
    //       expect(result.steps).toMatchSnapshot();
    //     });

    //     it('onStepFinish should be called for each step', () => {
    //       expect(onStepFinishResults).toMatchSnapshot();
    //     });
    //   });

    //   describe('2 steps: initial, tool-result with prepareStep', () => {
    //     let result: GenerateTextResult<any, any>;
    //     let onStepFinishResults: StepResult<any>[];

    //     beforeEach(async () => {
    //       onStepFinishResults = [];

    //       let responseCount = 0;

    //       const trueModel = new MockLanguageModelV1({
    //         // doGenerate: async ({ prompt, mode }) => {
    //         //   switch (responseCount++) {
    //         //     case 0:
    //         //       expect(mode).toStrictEqual({
    //         //         type: 'regular',
    //         //         toolChoice: { type: 'tool', toolName: 'tool1' },
    //         //         tools: [
    //         //           {
    //         //             type: 'function',
    //         //             name: 'tool1',
    //         //             description: undefined,
    //         //             parameters: {
    //         //               $schema: 'http://json-schema.org/draft-07/schema#',
    //         //               additionalProperties: false,
    //         //               properties: { value: { type: 'string' } },
    //         //               required: ['value'],
    //         //               type: 'object',
    //         //             },
    //         //           },
    //         //         ],
    //         //       });
    //         //       expect(prompt).toStrictEqual([
    //         //         {
    //         //           role: 'user',
    //         //           content: [{ type: 'text', text: 'test-input' }],
    //         //           providerMetadata: undefined,
    //         //         },
    //         //       ]);
    //         //       return {
    //         //         ...dummyResponseValues,
    //         //         toolCalls: [
    //         //           {
    //         //             toolCallType: 'function',
    //         //             toolCallId: 'call-1',
    //         //             toolName: 'tool1',
    //         //             args: `{ "value": "value" }`,
    //         //           },
    //         //         ],
    //         //         toolResults: [
    //         //           {
    //         //             toolCallId: 'call-1',
    //         //             toolName: 'tool1',
    //         //             args: { value: 'value' },
    //         //             result: 'result1',
    //         //           },
    //         //         ],
    //         //         finishReason: 'tool-calls',
    //         //         usage: { completionTokens: 5, promptTokens: 10 },
    //         //         response: {
    //         //           id: 'test-id-1-from-model',
    //         //           timestamp: new Date(0),
    //         //           modelId: 'test-response-model-id',
    //         //         },
    //         //       };
    //         //     case 1:
    //         //       expect(mode).toStrictEqual({
    //         //         type: 'regular',
    //         //         toolChoice: { type: 'auto' },
    //         //         tools: [],
    //         //       });
    //         //       expect(prompt).toStrictEqual([
    //         //         {
    //         //           role: 'user',
    //         //           content: [{ type: 'text', text: 'test-input' }],
    //         //           providerMetadata: undefined,
    //         //         },
    //         //         {
    //         //           role: 'assistant',
    //         //           content: [
    //         //             {
    //         //               type: 'tool-call',
    //         //               toolCallId: 'call-1',
    //         //               toolName: 'tool1',
    //         //               args: { value: 'value' },
    //         //               providerMetadata: undefined,
    //         //             },
    //         //           ],
    //         //           providerMetadata: undefined,
    //         //         },
    //         //         {
    //         //           role: 'tool',
    //         //           content: [
    //         //             {
    //         //               type: 'tool-result',
    //         //               toolCallId: 'call-1',
    //         //               toolName: 'tool1',
    //         //               result: 'result1',
    //         //               content: undefined,
    //         //               isError: undefined,
    //         //               providerMetadata: undefined,
    //         //             },
    //         //           ],
    //         //           providerMetadata: undefined,
    //         //         },
    //         //       ]);
    //         //       return {
    //         //         ...dummyResponseValues,
    //         //         text: 'Hello, world!',
    //         //         response: {
    //         //           id: 'test-id-2-from-model',
    //         //           timestamp: new Date(10000),
    //         //           modelId: 'test-response-model-id',
    //         //         },
    //         //         rawResponse: {
    //         //           headers: {
    //         //             'custom-response-header': 'response-header-value',
    //         //           },
    //         //         },
    //         //       };
    //         //     default:
    //         //       throw new Error(`Unexpected response count: ${responseCount}`);
    //         //   }
    //         // },
    //       });

    //       result = await generateText({
    //         model: modelWithFiles,
    //         tools: {
    //           tool1: tool({
    //             parameters: z.object({ value: z.string() }),
    //             execute: async (args, options) => {
    //               expect(args).toStrictEqual({ value: 'value' });
    //               expect(options.messages).toStrictEqual([{ role: 'user', content: 'test-input' }]);
    //               return 'result1';
    //             },
    //           }),
    //         },
    //         prompt: 'test-input',
    //         maxSteps: 3,
    //         // TODO: onStepFinish not implemented
    //         // onStepFinish: async event => {
    //         //   onStepFinishResults.push(event);
    //         // },
    //         // TODO: experimental_prepareStep not implemented
    //         // experimental_prepareStep: async ({ model, stepNumber, steps }) => {
    //         //   expect(model).toStrictEqual(modelWithFiles);

    //         //   if (stepNumber === 0) {
    //         //     expect(steps).toStrictEqual([]);
    //         //     return {
    //         //       model: trueModel,
    //         //       toolChoice: {
    //         //         type: 'tool',
    //         //         toolName: 'tool1' as const,
    //         //       },
    //         //     };
    //         //   }

    //         //   if (stepNumber === 1) {
    //         //     expect(steps.length).toStrictEqual(1);
    //         //     return { model: trueModel, experimental_activeTools: [] };
    //         //   }
    //         // },
    //         experimental_generateMessageId: mockId({ prefix: 'msg' }),
    //       });
    //     });

    //     it('result.text should return text from last step', async () => {
    //       assert.deepStrictEqual(result.text, 'Hello, world!');
    //     });

    //     it('result.toolCalls should return empty tool calls from last step', async () => {
    //       assert.deepStrictEqual(result.toolCalls, []);
    //     });

    //     it('result.toolResults should return empty tool results from last step', async () => {
    //       assert.deepStrictEqual(result.toolResults, []);
    //     });

    //     it('result.response.messages should contain response messages from all steps', () => {
    //       expect(result.response.messages).toMatchSnapshot();
    //     });

    //     it('result.usage should sum token usage', () => {
    //       assert.deepStrictEqual(result.usage, {
    //         completionTokens: 25,
    //         promptTokens: 20,
    //         totalTokens: 45,
    //       });
    //     });

    //     it('result.steps should contain all steps', () => {
    //       expect(result.steps).toMatchSnapshot();
    //     });

    //     it('onStepFinish should be called for each step', () => {
    //       expect(onStepFinishResults).toMatchSnapshot();
    //     });
    //   });

    //   describe('4 steps: initial, continue, continue, continue', () => {
    //     let result: GenerateTextResult<any, any>;
    //     let onStepFinishResults: StepResult<any>[];

    //     beforeEach(async () => {
    //       onStepFinishResults = [];

    //       let responseCount = 0;
    //       result = await generateText({
    //         model: new MockLanguageModelV1({
    //           doGenerate: async ({ prompt, mode }) => {
    //             switch (responseCount++) {
    //               case 0: {
    //                 expect(mode).toStrictEqual({
    //                   type: 'regular',
    //                   toolChoice: undefined,
    //                   tools: undefined,
    //                 });

    //                 expect(prompt).toStrictEqual([
    //                   {
    //                     role: 'user',
    //                     content: [{ type: 'text', text: 'test-input' }],
    //                     providerMetadata: undefined,
    //                   },
    //                 ]);

    //                 return {
    //                   ...dummyResponseValues,
    //                   // trailing text is to be discarded, trailing whitespace is to be kept:
    //                   text: 'part 1 \n to-be-discarded',
    //                   finishReason: 'length', // trigger continue
    //                   usage: { completionTokens: 20, promptTokens: 10 },
    //                   response: {
    //                     id: 'test-id-1-from-model',
    //                     timestamp: new Date(0),
    //                     modelId: 'test-response-model-id',
    //                   },
    //                 };
    //               }
    //               case 1: {
    //                 expect(mode).toStrictEqual({
    //                   type: 'regular',
    //                   toolChoice: undefined,
    //                   tools: undefined,
    //                 });

    //                 expect(prompt).toStrictEqual([
    //                   {
    //                     role: 'user',
    //                     content: [{ type: 'text', text: 'test-input' }],
    //                     providerMetadata: undefined,
    //                   },
    //                   {
    //                     role: 'assistant',
    //                     content: [
    //                       {
    //                         type: 'text',
    //                         text: 'part 1 \n ',
    //                         providerMetadata: undefined,
    //                       },
    //                     ],
    //                     providerMetadata: undefined,
    //                   },
    //                 ]);

    //                 return {
    //                   ...dummyResponseValues,
    //                   // case where there is no leading nor trailing whitespace:
    //                   text: 'no-whitespace',
    //                   finishReason: 'length',
    //                   response: {
    //                     id: 'test-id-2-from-model',
    //                     timestamp: new Date(10000),
    //                     modelId: 'test-response-model-id',
    //                   },
    //                   sources: [
    //                     {
    //                       sourceType: 'url' as const,
    //                       id: '123',
    //                       url: 'https://example.com',
    //                       title: 'Example',
    //                       providerMetadata: { provider: { custom: 'value' } },
    //                     },
    //                   ],
    //                   files: [
    //                     {
    //                       data: new Uint8Array([1, 2, 3]),
    //                       mimeType: 'image/png',
    //                       filename: 'test.png',
    //                     },
    //                   ],
    //                   usage: { completionTokens: 5, promptTokens: 30 },
    //                   // test handling of custom response headers:
    //                   rawResponse: {
    //                     headers: {
    //                       'custom-response-header': 'response-header-value',
    //                     },
    //                   },
    //                 };
    //               }
    //               case 2: {
    //                 expect(mode).toStrictEqual({
    //                   type: 'regular',
    //                   toolChoice: undefined,
    //                   tools: undefined,
    //                 });
    //                 expect(prompt).toStrictEqual([
    //                   {
    //                     role: 'user',
    //                     content: [{ type: 'text', text: 'test-input' }],
    //                     providerMetadata: undefined,
    //                   },
    //                   {
    //                     role: 'assistant',
    //                     content: [
    //                       {
    //                         type: 'text',
    //                         text: 'part 1 \n ',
    //                         providerMetadata: undefined,
    //                       },
    //                       {
    //                         type: 'text',
    //                         text: 'no-whitespace',
    //                         providerMetadata: undefined,
    //                       },
    //                     ],
    //                     providerMetadata: undefined,
    //                   },
    //                 ]);

    //                 return {
    //                   ...dummyResponseValues,
    //                   // set up trailing whitespace for next step:
    //                   text: 'immediatefollow  ',
    //                   finishReason: 'length',
    //                   sources: [
    //                     {
    //                       sourceType: 'url' as const,
    //                       id: '456',
    //                       url: 'https://example.com/2',
    //                       title: 'Example 2',
    //                       providerMetadata: { provider: { custom: 'value2' } },
    //                     },
    //                     {
    //                       sourceType: 'url' as const,
    //                       id: '789',
    //                       url: 'https://example.com/3',
    //                       title: 'Example 3',
    //                       providerMetadata: { provider: { custom: 'value3' } },
    //                     },
    //                   ],
    //                   response: {
    //                     id: 'test-id-3-from-model',
    //                     timestamp: new Date(20000),
    //                     modelId: 'test-response-model-id',
    //                   },
    //                   usage: { completionTokens: 2, promptTokens: 3 },
    //                 };
    //               }
    //               case 3: {
    //                 expect(mode).toStrictEqual({
    //                   type: 'regular',
    //                   toolChoice: undefined,
    //                   tools: undefined,
    //                 });
    //                 expect(prompt).toStrictEqual([
    //                   {
    //                     role: 'user',
    //                     content: [{ type: 'text', text: 'test-input' }],
    //                     providerMetadata: undefined,
    //                   },
    //                   {
    //                     role: 'assistant',
    //                     content: [
    //                       {
    //                         type: 'text',
    //                         text: 'part 1 \n ',
    //                         providerMetadata: undefined,
    //                       },
    //                       {
    //                         type: 'text',
    //                         text: 'no-whitespace',
    //                         providerMetadata: undefined,
    //                       },
    //                       {
    //                         type: 'text',
    //                         text: 'immediatefollow  ',
    //                         providerMetadata: undefined,
    //                       },
    //                     ],
    //                     providerMetadata: undefined,
    //                   },
    //                 ]);

    //                 return {
    //                   ...dummyResponseValues,
    //                   // leading whitespace is to be discarded when there is whitespace from previous step
    //                   // (for models such as Anthropic that trim trailing whitespace in their inputs):
    //                   text: '  final value keep all whitespace\n end',
    //                   finishReason: 'stop',
    //                   files: [
    //                     {
    //                       data: 'QkFVRw==',
    //                       mimeType: 'image/jpeg',
    //                       filename: 'test.jpeg',
    //                     },
    //                   ],
    //                   response: {
    //                     id: 'test-id-4-from-model',
    //                     timestamp: new Date(20000),
    //                     modelId: 'test-response-model-id',
    //                   },
    //                   usage: { completionTokens: 2, promptTokens: 3 },
    //                 };
    //               }
    //               default:
    //                 throw new Error(`Unexpected response count: ${responseCount}`);
    //             }
    //           },
    //         }),
    //         prompt: 'test-input',
    //         maxSteps: 5,
    //         experimental_continueSteps: true,
    //         onStepFinish: async event => {
    //           onStepFinishResults.push(event);
    //         },
    //         experimental_generateMessageId: mockId({ prefix: 'msg' }),
    //       });
    //     });

    //     it('result.text should return text from both steps separated by space', async () => {
    //       expect(result.text).toStrictEqual(
    //         'part 1 \n no-whitespaceimmediatefollow  final value keep all whitespace\n end',
    //       );
    //     });

    //     it('result.response.messages should contain an assistant message with the combined text', () => {
    //       expect(result.response.messages).toStrictEqual([
    //         {
    //           role: 'assistant',
    //           id: 'msg-0',
    //           content: [
    //             {
    //               text: 'part 1 \n ',
    //               type: 'text',
    //             },
    //             {
    //               text: 'no-whitespace',
    //               type: 'text',
    //             },
    //             {
    //               text: 'immediatefollow  ',
    //               type: 'text',
    //             },
    //             {
    //               text: 'final value keep all whitespace\n end',
    //               type: 'text',
    //             },
    //           ],
    //         },
    //       ]);
    //     });

    //     it('result.usage should sum token usage', () => {
    //       expect(result.usage).toStrictEqual({
    //         completionTokens: 29,
    //         promptTokens: 46,
    //         totalTokens: 75,
    //       });
    //     });

    //     it('result.steps should contain all steps', () => {
    //       expect(result.steps).toMatchSnapshot();
    //     });

    //     it('onStepFinish should be called for each step', () => {
    //       expect(onStepFinishResults).toMatchSnapshot();
    //     });

    //     it('result.sources should contain sources from all steps', () => {
    //       expect(result.sources).toMatchSnapshot();
    //     });

    //     it('result.files should contain files from last step', () => {
    //       expect(result.files).toMatchSnapshot();
    //     });
    //   });
    // });
  });
}
